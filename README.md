# **Leaffliction üåø**

‚ö†Ô∏è **Warning:**
This README was **partially generated by ChatGPT-o3**. While effort has been made to ensure accuracy, some details may be incorrect or require further verification.
Please **double-check the information**.

---

## **Project Overview**
This project focus on building computer-vision pipeline that balances an imbalanced dataset,
performs a few image transformations + a colour-histogram extraction, and trains a neural network
to reach ‚â• 90 % accuracy on a validation set composed of at least 100 held-out images.

Subject brief: see **leaffliction.pdf**.

## **Objectives**
The primary objectives of this project are:

‚úÖ **Analyze a raw dataset:** Learn to apprehend data imbalance and display relevant information on dataset.  
‚úÖ **Balance the classes via data augmentation:** Learn to balance the dataset by generating new data using techniques such as **data augmentation**.  
‚úÖ **Extract information from images :** Use **image transformations** to extract relevant features from images.  
‚úÖ **Train a multimodal classifier neural network:** Build and train a neural network to classify images, achieving at least 90% accuracy on a validation set.  
‚úÖ **Provide an inference CLI:** Create a prediction method using the trained model. Also used to assess the performance of the trained model on a test set and analyze its accuracy.  


This project serves as an introduction to **computer vision** concepts. We also used it as a TensorFlow learning exercise.

---

## **Project Documentation**

[Auto-generated documentation from Docstrings](https://brenaudon.github.io/Leaffliction/)

---

## **Setup Project**

To set up the project, follow these steps:
1. **Clone the repository:**
   ```bash
    git clone git@github.com:brenaudon/Leaffliction.git Leaffliction
    cd Leaffliction
    ```
2. **Install dependencies:**
Optionnally, you can create a virtual environment to isolate the project dependencies.
   ```bash
    python -m venv venv
    source venv/bin/activate  # Adapt this to your distribution
   ```
   Then, install the required packages:
   ```bash
   pip install -r requirements.txt
   ```

3. **Download the dataset:**
Dataset is not included in the repository due to its size. You can download it from the following link:
   - [Leaffliction Dataset](https://cdn.intra.42.fr/document/document/17060/leaves.zip) (**Note:** This link may change, please refer to the project page for the latest link). 

   Unzip the dataset in the root directory of the project.
---

## **Usage**

This project consists of multiple scripts designed to visualize data, preprocess data, train a neural network, and make predictions. Below are the usage instructions for each script.  
Scripts should be run from the root directory of the project to ensure correct file paths.

### üîπ **1. Visualizing Data Distribution**
To explore the dataset visually, use the `Distribution.py` script.

```bash
python srcs/Distribution.py <path_to_directory>
```
Directory should contain subdirectories for each class, with images inside.

This script display:
- **Pie chart** to show class distribution.
- **Bar chart** to show the number of images per class.

### üîπ **2. Balancing the Dataset**
Before training, the dataset should be balanced to ensure fair representation of each class.

```bash
python srcs/Augmentation.py <path_to_image | path_to_directory>
```
- `<path_to_image>`: Path to the image to augment. Will apply all augmentations to this image and display them.
- `<path_to_directory>`: Path to a directory containing subdirectories for each class, with images inside. Will apply augmentations to balance number of images in each subdirectory.

### üîπ **3. Transformations**
To apply transformations to images, use the `Transformations.py` script.

**Transformations on a single image**

```bash
python srcs/Transformation.py <path_to_image>
```
- `<path_to_image>`: Path to the image to transform. Will apply all transformations to this image and display them.

**Transformation on a directory of images**

```bash
python srcs/Transformation.py -src <source_directory> -dst <destination_directory> [tags]
```

- `<source_directory>`: Path to a directory containing images. Will apply transformations to all images in the directory.
- `<destination_directory>`: Path to the directory where transformed images will be saved.
- `[tags]`: Optional tags to define which transformations to apply. If not specified, all transformations will be applied. 
  Available tags: `-all`, `-gaussian`, `-mask`, `-analyze`, `-roi`, `-landmark`, `-hist`. Please refer to **Transformations** section for more details on each transformation.

### üîπ **4. Preprocess dataset**
Can be used to preprocess the dataset before training the model. This script will apply transformations and save them in cache.
Not supposed to be run manually, as it is called by the training script. If run manually, training script will only use the preprocessed images if saved in cache/prepared directory.

```bash
python srcs/preprocess_dataset.py -i <input_directory> [-o <output_directory>]
```
- `-i, --input-dir` : Path to the input directory containing subdirectories for each class, with images inside.
- `-o, --output-dir` : Optional path to the output directory where preprocessed images will be saved. If not specified, it will save in a default cache directory (cache/prepared).

### üîπ **5. Train neural network**
To start or resume the training of the neural network, use the `train.py` script.

```bash
python srcs/train.py -i <input_directory> [-m <model>] [-e <epochs>]
```
- `-i, --input-dir` : Path to the input directory containing subdirectories for each class, with images inside.
- `-m, --model` : Optional path to a pre-trained model to resume training. If not specified, it will train a new model. Previous validation set will be retrieved from cache (`cache/val_split.txt`).
- `-e, --epochs` : Optional number of epochs to train the model **before asking user input to continue**. If not specified, it will use the default value (1).

### üîπ **6. Predict a leaf class**
To predict the class of a leaf image using the trained model, use the `predict.py` script.

```bash
python srcs/predict.py <image_path | directory_path | txt_file_with_image_paths>
```
- `<image_path>` : Path to a single image to predict its class.
- `<directory_path>` : Path to a directory containing images to predict their classes. Will predict all images in the directory (recursive search).
- `<txt_file_with_image_paths>` : Path to a text file containing paths to images to predict their classes. Each line should contain a path to an image.


### üîπ **7. Save model and cache in ZIP**
As asked in the subject brief, you can save the model and cache in a ZIP file for submission. signature.txt file containing sha1sum of the ZIP file is generated.
Should not be run manually, as it is called by the training script. Cache has to be saved in `cache/` directory, and model in `model/` directory.

```bash
python srcs/save_zip.py
```

### üîπ **8. Helper**
Delete every image containing a "_" in their filename to go back to the original dataset.

```bash
python srcs/helper.py <path_to_directory>
```
- `<path_to_directory>`: Path to the directory containing images. Will delete all images containing "_" in their filename (recursive search).

---

## **Data Distribution**

Distribution script display a pie chart and a bar chart to visualize data distribution. Distribution of the given dataset is as follows:

![Distribution Chart](https://github.com/brenaudon/Leaffliction/blob/master/doc/distribution.png)

## **Data Augmentation**

Data augmentation script applies various augmentations to images to balance the dataset. The following augmentations are applied:
- **Blur**: Applies a Gaussian blur to the image.
- **Flip**: Flips the image horizontally.
- **Brightness**: Adjusts the brightness of the image (can become darker or brighter).
- **Crop**: Crops the image by a random factor between 0.8 and 0.95.
- **Distortion**: Distorts the image.
- **Grid Distortion**: Applies a grid distortion to the image. Interesting for organic shapes like leaves because it preserves the overall aspect of a leaf while locally changing its shape.

Here is an example of the augmentations applied to a single image:

![Augmentation Example](https://github.com/brenaudon/Leaffliction/blob/master/doc/augmentation_example.png)


Distribution of the dataset after augmentation is as follows:

![Augmented Distribution Chart](https://github.com/brenaudon/Leaffliction/blob/master/doc/augmented_distribution.png)

## **Data Transformation**

Transformation script applies various transformations to images to extract relevant features. The following transformations are applied:
- **Gaussian**: Applies a Gaussian blur to the image.
- **Mask**: Applies a mask to the image to only keep the leaf.
- **ROI**: Extracts the region of interest (ROI) from the image.
- **Analyze**: "Analyzes" the image by tracing the leaf contours, length, width, centroid and principal axis.
- **Landmark**: Applies a landmark detection to the image.
- **Histogram**: Extracts the color histogram of the image.

To do so the script uses PlantCV library, which is a computer vision library for plant phenotyping. It provides various functions to analyze plant images and extract relevant features.

Here is an example of the transformations applied to a single image:

![Transformation Example](https://github.com/brenaudon/Leaffliction/blob/master/doc/transformation_example.png)  

![Color Histogram Example](https://github.com/brenaudon/Leaffliction/blob/master/doc/histogram_example.png)

## **Neural Network**

The neural network is a multimodal classifier that takes as input the transformed images and the color histogram of the images. It is built using TensorFlow and Keras.

### **Model Architecture**

A naive model would have been to use a CNN on the transformed images and give each transformed image as a different input to the model.
However, in order to keep the link between original image and transformed images, we decided to use a **Siamese Network** architecture.
For color histogram, we use a simple MLP to extract features from the histogram and then concatenate the features with the features extracted from the transformed images (**late / feature-level fusion**).

**Siamese Network** imply using an identical subnetworks that share the same weights and parameters on several inputs with the same shapes (here the original image and the transformed ones).  
In our case, we use a **CNN (Convolutional Neural Network)** as the subnetwork to extract features from the images.

After extracting features from the transformed images, we concatenate them with the features extracted from the color histogram.  
Then, we use a **fully connected layer** to classify the images into one of the classes.

Here is the architecture of the model:  
![Model Architecture](https://github.com/brenaudon/Leaffliction/blob/master/doc/model_architecture.png)

### **Convolutional Neural Network (CNN)**

The CNN is used to extract features from the transformed images. It consists of 2 pairs of a **convolutional layer** followed by **max pooling layer** then a **flatten layer** to convert the 2D feature maps into a 1D vector and finally a **dense layer** to extract features from the images.  
Convolutional layers are used to **extract local features** from the images by applying **filters** to the images. It results in a set of **feature maps** that represent the presence of certain features in the images.  
Max pooling layers are used to **downsample the feature maps** and **reduce the number of parameters** in the model. Downsampling is done by taking the maximum value in a certain region of the feature maps (average pooling also exists).

Here is the architecture of the CNN shared by the Siamese Network:  
![CNN Architecture](https://github.com/brenaudon/Leaffliction/blob/master/doc/cnn_architecture.png)

### **Training**

The model is trained using the **Adam optimizer** and the **categorical crossentropy** loss function.  
The model is trained on the augmented dataset with a **batch size of 8** (unstable with larger batch size due to memory constraints, could get killed by zsh).  
As epochs are pretty long (around 65 minutes for a batch size of 8, 45 minutes for batch size of 16), we decided to ask **user input** to continue training after each epoch.
An option is available to specify the number of epochs to train before asking user input to continue.

Training can be **resumed** by specifying a pre-trained model. In this case, the model will be trained on the **validation set** split from the **cache directory** (`cache/val_split.txt`).
Indeed, using a new split would be **problematic** as some images from the new validation set would have been previously used to train the model, leading to a **biased evaluation**.

### **Input pipeline**

We chose to apply the transformations and extract the color histogram of the images **before training**.  
These preprocessed images are saved in a cache directory (`cache/prepared/`) to avoid applying the transformations and extracting the color histogram again during training.  
The input pipeline is built using **TensorFlow's data API**. It reads the images and histogram from the cache directory, checks shapes and adapt it if necessary.

Using TensorFlow's data API allows us to efficiently load and preprocess the images in **parallel** using **multiple CPU cores**.

### **Evaluation**

The model is evaluated on the validation set after each epoch. The evaluation metric is the **accuracy** of the model on the validation set.  
Train / validation split is 90% / 10% of the dataset.
The model is saved in the `model/` directory after each epoch if the accuracy is improved.

### **Prediction**

To predict the class of a leaf image, the trained model is used to classify the image.
The prediction is done by **applying the same transformations** as during training and then passing 
the transformed images and the color histogram through the model.

Prediction can be done on a single image, a directory of images or a text file containing paths to images.  
If done on several images, results will be displayed in terminal.  
If done on a single image, the predicted class will be displayed as asked in the subject brief :  
![Predicted Class Example](https://github.com/brenaudon/Leaffliction/blob/master/doc/predicted_class_example.png)

### **Zipping Model and Cache**

The model is also saved in a **ZIP file** with the cache directory for submission. The ZIP file contains:
- The **model** in `model/` directory.
- The **cache directory** in `cache/` directory.  

A `signature.txt` file containing the SHA1 checksum of the ZIP file is also generated.
